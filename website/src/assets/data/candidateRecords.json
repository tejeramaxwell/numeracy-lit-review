{"papers":[{"url":"https://www.semanticscholar.org/paper/5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":213,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2023","authors":"Wentao Liu,Hanglei Hu,Jie Zhou,Yuyang Ding,Junsong Li,Jiayi Zeng,Mengliang He,Qin Chen,Bo Jiang,Aimin Zhou,Liang He","id":"5ee871537ae51e7e2e93d2a70fff5d100649a655","summary":"A comprehensive survey of mathematical LMs is conducted, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies, revealing a large number of proposed mathematical LLMs.","score":12},{"url":"https://www.semanticscholar.org/paper/42445823fb0156afddc8c72eaa5ee81dded5b965","title":"Large Language Models for Mathematical Reasoning: Progresses and Challenges","venue":"","year":2024,"referenceCount":92,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2024","authors":"Janice Ahn,Rishu Verma,Renze Lou,Di Liu,Rui Zhang,Wenpeng Yin","id":"42445823fb0156afddc8c72eaa5ee81dded5b965","summary":"This survey stands as one of the first extensive examinations of the landscape of LLM-oriented techniques in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.","score":10},{"url":"https://www.semanticscholar.org/paper/5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2023","authors":"Mubashara Akhtar,Abhilash Shankarampeta,Vivek Gupta,Arpit Patil,O. Cocarascu,Elena Simperl","id":"5be5619fc22300ef356ec4ef729d567ce7116c57","summary":"A hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning is proposed.","score":9},{"url":"https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":70,"citationCount":78,"influentialCitationCount":6,"publicationDate":"24/03/2021","authors":"Avijit Thawani,J. Pujara,Pedro A. Szekely,Filip Ilievski","id":"28a5a53dafacebad8a7c47773079caeffb9a5baa","summary":"This work synthesizes best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.","score":9},{"url":"https://www.semanticscholar.org/paper/fee61c03fd5ad4a8d8bcdec5bcdfacfe25b361d9","title":"Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges","venue":"arXiv.org","year":2023,"referenceCount":96,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/12/2023","authors":"Qingyao Li,Lingyue Fu,Weiming Zhang,Xianyu Chen,Jingwei Yu,Wei Xia,Weinan Zhang,Ruiming Tang,Yong Yu","id":"fee61c03fd5ad4a8d8bcdec5bcdfacfe25b361d9","summary":"This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim of exploring their potential in constructing the next-generation intelligent education system.","score":7},{"url":"https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":219,"citationCount":109,"influentialCitationCount":3,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","score":7},{"url":"https://www.semanticscholar.org/paper/b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/09/2023","authors":"Eran Malach","id":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","summary":"This work presents a theoretical framework for studying auto-regressive next-token predictors, and introduces a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and examines the interplay between length complexity and other notions of complexity.","score":7},{"url":"https://www.semanticscholar.org/paper/dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2023","authors":"Changnan Xiao,Bing Liu","id":"dd69049674f41d4ef5314b8f95bacfe59de31376","summary":"This work identifies and proves conditions that decide whether the length generalization problem can be solved or not for a reasoning task in a particular representation.","score":7},{"url":"https://www.semanticscholar.org/paper/9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation","venue":"arXiv.org","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/08/2023","authors":"D. Maltoni,M. Ferrara","id":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","summary":"The findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.","score":6},{"url":"https://www.semanticscholar.org/paper/4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/11/2023","authors":"Ruoqi Shen,Sébastien Bubeck,Ronen Eldan,Yin Tat Lee,Yuanzhi Li,Yi Zhang","id":"4726d1dc54851db99c29180127d840bd19f20afc","summary":"This work delve deeper into the role of positional encoding, and proposes several ways to fix the issue, either by modifying the positional encoding directly, or by modified the representation of the arithmetic task to leverage standard positional encoding differently.","score":6},{"url":"https://www.semanticscholar.org/paper/174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression","venue":"arXiv.org","year":2023,"referenceCount":17,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2023","authors":"Avijit Thawani,Jay Pujara,Ashwin Kalyan","id":"174a9b78350e9561555052bc6901cc44782f4c62","summary":"A carefully designed tokenization scheme is both the simplest to implement and sufficient, with similar performance to the state-of-the-art approach that requires making significant architectural changes, in the context of masked number prediction.","score":6},{"url":"https://www.semanticscholar.org/paper/18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/12/2023","authors":"Yuncheng Huang,Qi He,Jiaqing Liang,Sihang Jiang,Yanghua Xiao,Yunwen Chen","id":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","summary":"This work presents a framework to enhance the quantitative reasoning ability of language models based on dimension perception and proposes a benchmark DimEval consisting of seven tasks of three categories to probe and enhance the dimension perception skills of LLMs.","score":6},{"url":"https://www.semanticscholar.org/paper/0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":19,"influentialCitationCount":4,"publicationDate":"07/07/2023","authors":"Nayoung Lee,Kartik K. Sreenivasan,Jason D. Lee,Kangwook Lee,Dimitris Papailiopoulos","id":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","summary":"This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective.","score":6},{"url":"https://www.semanticscholar.org/paper/a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":60,"citationCount":26,"influentialCitationCount":8,"publicationDate":"21/05/2023","authors":"Wenhu Chen,Ming Yin,Max W.F. Ku,Yixin Wan,Xueguang Ma,Jianyu Xu,Tony Xia,Xinyi Wang,Pan Lu","id":"a52dd1e900200e0733eea927edc7d6c27aeba187","summary":"This paper introduces TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems and finds that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting.","score":5},{"url":"https://www.semanticscholar.org/paper/8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":36,"citationCount":5,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji","id":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","summary":"The proposed CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization, disentangles abstract tool creation and concrete decision execution, resulting in improved performance.","score":5},{"url":"https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7","title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct","venue":"arXiv.org","year":2023,"referenceCount":107,"citationCount":65,"influentialCitationCount":14,"publicationDate":"18/08/2023","authors":"Haipeng Luo,Qingfeng Sun,Can Xu,Pu Zhao,Jian-Guang Lou,Chongyang Tao,Xiubo Geng,Qingwei Lin,Shifeng Chen,Dongmei Zhang","id":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","summary":"WizardMath is presented, which enhances the mathematical reasoning abilities of Llama-2, by applying the proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math.","score":5},{"url":"https://www.semanticscholar.org/paper/2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/08/2023","authors":"Dingzirui Wang,Longxu Dou,Wenbin Zhang,Junyu Zeng,Wanxiang Che","id":"2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","summary":"This paper presents a method called Boosting Numerical Reason\\textbfing by Decomposing the Generation of Equations (Bridge), which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs.","score":5},{"url":"https://www.semanticscholar.org/paper/2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","title":"Design of Chain-of-Thought in Math Problem Solving","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":4,"influentialCitationCount":1,"publicationDate":"20/09/2023","authors":"Zhanming Jie,Trung Quoc Luong,Xinbo Zhang,Xiaoran Jin,Hang Li","id":"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","summary":"Through extensive experiments on GSM8K, MATHQA, and SVAMP, it is found that program CoTs often have superior effectiveness in math problem solving and Python is a better choice of language than Wolfram for program CoT.","score":5},{"url":"https://www.semanticscholar.org/paper/b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":"08/10/2023","authors":"Cheng Qian,Chenyan Xiong,Zhenghao Liu,Zhiyuan Liu","id":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","summary":"This paper introduces Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-s solving (CoS) approach, and results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities.","score":5},{"url":"https://www.semanticscholar.org/paper/f176d0d466d7c778a6435fe9a8d7e49508cb9059","title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":41,"citationCount":4,"influentialCitationCount":0,"publicationDate":"23/10/2023","authors":"Tengxiao Liu,Qipeng Guo,Yuqing Yang,Xiangkun Hu,Yue Zhang,Xipeng Qiu,Zheng Zhang","id":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","summary":"XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts by allowing method switching, provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework.","score":5},{"url":"https://www.semanticscholar.org/paper/6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents","venue":"arXiv.org","year":2023,"referenceCount":182,"citationCount":7,"influentialCitationCount":0,"publicationDate":"20/11/2023","authors":"Zhuosheng Zhang,Yao Yao,Aston Zhang,Xiangru Tang,Xinbei Ma,Zhiwei He,Yiming Wang,Mark B. Gerstein,Rui Wang,Gongshen Liu,Hai Zhao","id":"6fa0677731184444df0e1fc8070938419cd6da47","summary":"A thorough discourse, penetrating vital research dimensions, encompassing the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; the paradigm shift in CoT; and the burgeoning of language agents fortified by CoT approaches.","score":5},{"url":"https://www.semanticscholar.org/paper/c50162023fe8e9347401e24fda437c01eb44f8b9","title":"From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2023","authors":"Nuo Chen,Hongguang Li,Baoyuan Wang,Jia Li","id":"c50162023fe8e9347401e24fda437c01eb44f8b9","summary":"Experimental analysis shows that IMP-TIP achieves enhanced mathematical capabilities and outperforms traditional LLMs and tool-augmented LLMs in accuracy and reasoning diversity on math reasoning tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":4,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao","id":"3693683c4e0405819fae7115ad680f769eb83534","summary":"The method, which call \"Neural Comprehension\", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning.","score":5},{"url":"https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning","venue":"arXiv.org","year":2022,"referenceCount":68,"citationCount":65,"influentialCitationCount":10,"publicationDate":"15/11/2022","authors":"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi","id":"4d17732d90440682b0500f4e209c6cc4fac20e0e","summary":"This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates significant boosts in performance.","score":5},{"url":"https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models","venue":"International Conference on Machine Learning","year":2022,"referenceCount":67,"citationCount":227,"influentialCitationCount":44,"publicationDate":"18/11/2022","authors":"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig","id":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","summary":"This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.","score":5},{"url":"https://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":96,"citationCount":38,"influentialCitationCount":3,"publicationDate":2021,"authors":"Chadi Helwe,C. Clavel,Fabian M. Suchanek","id":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","summary":"This survey paper discusses the performance of transformers on diﬀerent reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.","score":5},{"url":"https://www.semanticscholar.org/paper/12d16f426edc6ab248fb476007bd1646282d4d68","title":"Language Model Behavior: A Comprehensive Survey","venue":"Computational Linguistics","year":2023,"referenceCount":392,"citationCount":21,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Tyler A. Chang,B. Bergen","id":"12d16f426edc6ab248fb476007bd1646282d4d68","summary":"Over 250 recent studies of English language model behavior before task-specific fine-tuning are discussed, synthesizing recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.","score":5},{"url":"https://www.semanticscholar.org/paper/c77feebb06b530079a34f04c64de359ae28c0a32","title":"Predicting Numerals in Text Using Nearest Neighbor Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"T. Sakamoto,Akiko Aizawa","id":"c77feebb06b530079a34f04c64de359ae28c0a32","summary":null,"score":5},{"url":"https://www.semanticscholar.org/paper/5dc15ac1c92ab7492f121471823fb13a95d273ba","title":"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":51,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/05/2023","authors":"Alessandro Stolfo,Yonatan Belinkov,Mrinmaya Sachan","id":"5dc15ac1c92ab7492f121471823fb13a95d273ba","summary":"A mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework is presented and results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism.","score":4},{"url":"https://www.semanticscholar.org/paper/7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS","venue":"International Journal of Digital Earth","year":2023,"referenceCount":96,"citationCount":7,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"Zhenlong Li,H. Ning","id":"7a6dc7071891cb3d658c93418801942a4c6ed373","summary":"Although still in its infancy and lacking several important modules such as logging and code testing, LLM-Geo demonstrates a potential path toward the next-generation AI-powered GIS, and advocates for the GIScience community to devote more efforts to the research and development of autonomous GIS.","score":4},{"url":"https://www.semanticscholar.org/paper/36f46391b00a7eb4ffc991f964a36b264811057d","title":"Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks","venue":"","year":null,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Avinash Anand,Mohit Gupta,Kritarth Prasad,Navya Singla,Sanjana Sanjeev,Jatin Kumar,A. Shivam,Rajiv Ratn","id":"36f46391b00a7eb4ffc991f964a36b264811057d","summary":"An extensive mathematics dataset called \" MathQuest \" sourced from the 11th and 12th standard Mathematics NCERT textbooks is introduced, and MAmmoTH-13B establishes itself as a robust and dependable benchmark for addressing NCERT mathematics problems.","score":4},{"url":"https://www.semanticscholar.org/paper/fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving","venue":"Knowledge Discovery and Data Mining","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/06/2023","authors":"Wayne Xin Zhao,Kun Zhou,Beichen Zhang,Zheng Gong,Zhipeng Chen,Yuanhang Zhou,Ji-rong Wen,Jing Sha,Shijin Wang,Cong Liu,Guoping Hu","id":"fae57797d357bfa3b39b220336d1a2e8deba5318","summary":"The idea is to maintain a moderate-sized model and employ the cross-task knowledge sharing to improve the model capacity in a multi-task setting, and construct a Mixture-of-Experts (MoE) architecture for modeling mathematical text, to capture the common mathematical knowledge across tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":16,"influentialCitationCount":2,"publicationDate":"20/07/2023","authors":"Xiaoxuan Wang,Ziniu Hu,Pan Lu,Yanqiao Zhu,Jieyu Zhang,Satyen Subramaniam,Arjun R. Loomba,Shichang Zhang,Yizhou Sun,Wei Wang","id":"4993258852711c4e3d0011325ac3db680eae84f4","summary":"An expansive benchmark suite SciBench is introduced that aims to systematically examine the reasoning capabilities required for complex scientific problem solving in large language models and indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills.","score":4},{"url":"https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning","venue":"International Conference on Learning Representations","year":2022,"referenceCount":58,"citationCount":82,"influentialCitationCount":11,"publicationDate":"29/09/2022","authors":"Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan","id":"3e565c544a8639cc9c7568833e484d7610f5e5d4","summary":"A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.","score":4},{"url":"https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":130,"citationCount":171,"influentialCitationCount":5,"publicationDate":"20/12/2022","authors":"Jie Huang,K. Chang","id":"db4ab91d5675c37795e719e997a2827d3d83cd45","summary":"A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions are provided.","score":4},{"url":"https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":123,"influentialCitationCount":15,"publicationDate":"19/04/2023","authors":"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao","id":"170c97c7215f42edfb20c2248f954879e91ef86e","summary":"This paper demonstrates the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","score":4},{"url":"https://www.semanticscholar.org/paper/a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":77,"citationCount":49,"influentialCitationCount":5,"publicationDate":"11/09/2023","authors":"Xiang Yue,Xingwei Qu,Ge Zhang,Yao Fu,Wenhao Huang,Huan Sun,Yu Su,Wenhu Chen","id":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","summary":"The MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%, and underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.","score":4},{"url":"https://www.semanticscholar.org/paper/b272513916b45c8517d289d7abee4a53e6832187","title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":16,"influentialCitationCount":3,"publicationDate":"29/09/2023","authors":"Zhibin Gou,Zhihong Shao,Yeyun Gong,Yelong Shen,Yujiu Yang,Minlie Huang,Nan Duan,Weizhu Chen","id":"b272513916b45c8517d289d7abee4a53e6832187","summary":"This paper proposes ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools, thereby amalgamating the analytical prowess of language and the computational efficiency of tools.","score":4},{"url":"https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc","title":"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/10/2023","authors":"Jianpeng Zhou,Wanjun Zhong,Yanlin Wang,Jiahai Wang","id":"5076bbbf831a92174c9cc1b347bd0584560435fc","summary":"Experimental results from complex reasoning tasks reveal that the prompting method adaptation and decomposition granularity adaptation enhance performance across all tasks, and the model adaptation approach significantly reduces API costs while maintaining superior performance.","score":4},{"url":"https://www.semanticscholar.org/paper/b6667ba4f586489f12587446c6daaa3f09cfc539","title":"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2023","authors":"Haoyi Wu,Wenyang Hui,Yezeng Chen,Weiqi Wu,Kewei Tu,Yi Zhou","id":"b6667ba4f586489f12587446c6daaa3f09cfc539","summary":"This work proposes Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education that provides a high-quality formal representation, the reasoning steps, and the final solution, and experiments show that existing large language models exhibit weak performance on complex reasoning.","score":4},{"url":"https://www.semanticscholar.org/paper/eee5a8e06c32112d268e88a8d4c45592d7214244","title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/11/2023","authors":"Yilun Zhao,Yitao Long,Hongjun Liu,Linyong Nan,Lyuhao Chen,Ryo Kamoi,Yixin Liu,Xiangru Tang,Rui Zhang,Arman Cohan","id":"eee5a8e06c32112d268e88a8d4c45592d7214244","summary":"It is found that, although the current best-performing system, GPT-4, can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts.","score":4},{"url":"https://www.semanticscholar.org/paper/d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b","title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/11/2023","authors":"Yilun Zhao,Hongjun Liu,Yitao Long,Rui Zhang,Chen Zhao,Arman Cohan","id":"d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b","summary":"This study introduces KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems, and provides expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment.","score":4},{"url":"https://www.semanticscholar.org/paper/4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives","venue":"arXiv.org","year":2024,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/01/2024","authors":"Wenqi Zhang,Yongliang Shen,Linjuan Wu,Qiuying Peng,Jun Wang,Y. Zhuang,Weiming Lu","id":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","summary":"Self-Contrast is advocated: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies.","score":4},{"url":"https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension","venue":"arXiv.org","year":2022,"referenceCount":62,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ippei Fujisawa,R. Kanai","id":"965e409a3e7b5670d609837fac9823b160d6639c","summary":"This work defines and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","score":4},{"url":"https://www.semanticscholar.org/paper/8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes","venue":"medRxiv","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/07/2023","authors":"A. Soroush,B. Glicksberg,E. Zimlichman,Y. Barash,R. Freeman,A. Charney,G. Nadkarni,E. Klang","id":"8f1b0c247171510fc68da27a16a377456376a5a7","summary":"While the models appear to exhibit a general conceptual understanding of the codes and their descriptions, they have a propensity for hallucinating key details, suggesting underlying technological limitations of the base LLMs.","score":4},{"url":"https://www.semanticscholar.org/paper/0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration","venue":"arXiv.org","year":2021,"referenceCount":47,"citationCount":18,"influentialCitationCount":1,"publicationDate":"05/09/2021","authors":"Gabriel Recchia","id":"0f2199296f01694ee46b6059879260fb80a84fa6","summary":"The results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/856c342606aca05434e48f2e53cdbd6f6b886802","title":"Pre‐trained language models: What do they know?","venue":"WIREs Data Mining and Knowledge Discovery","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/09/2023","authors":"Nuno Guimarães,Ricardo Campos,Alípio M. Jorge","id":"856c342606aca05434e48f2e53cdbd6f6b886802","summary":"The behavior of pre‐trained language models (PLMs) in some inference tasks they were not initially trained for are studied to highlight relevant achievements made by these models, as well as some of their current limitations that open opportunities for further research.","score":4},{"url":"https://www.semanticscholar.org/paper/1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning","venue":"Trans. Mach. Learn. Res.","year":2023,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"David A. Klindt","id":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","summary":"A tight linkage between the scaling of a network weights’ standard deviation and its effective length scale on a sinusoidal regression problem is demonstrated, suggesting simple modifications to control the length scale of the function learned by a neural network and, thus, its smoothness.","score":4},{"url":"https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","venue":"ACM Computing Surveys","year":2021,"referenceCount":348,"citationCount":113,"influentialCitationCount":7,"publicationDate":"27/07/2021","authors":"Anna Rogers,Matt Gardner,Isabelle Augenstein","id":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","summary":"This study is the largest survey of the deep learning models in NLP field to date, providing an overview of the various formats and domains of the current resources, and highlighting the current lacunae for future work.","score":4},{"url":"https://www.semanticscholar.org/paper/2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey","venue":"ACM Computing Surveys","year":2023,"referenceCount":192,"citationCount":18,"influentialCitationCount":1,"publicationDate":"09/02/2023","authors":"E. Davis","id":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","summary":"A number of recommendations for future development of commonsense AI benchmarks are made, including that the creators of benchmarks invest the work needed to ensure that benchmark examples are consistently high quality.","score":4},{"url":"https://www.semanticscholar.org/paper/681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey","venue":"","year":2023,"referenceCount":391,"citationCount":8,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Xavier Daull,P. Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco","id":"681cee58cf7e54199191cf9e0baf6851d8356704","summary":"This paper reviews the state-of-the-art of language models architectures and strategies for \"complex\"question-answering (QA, C QA, CPS) with a focus on hybridization, and discusses some challenges associated with complex QA.","score":4},{"url":"https://www.semanticscholar.org/paper/d3d8f37a809a0e3efe1cf6419033ed36424ffc8a","title":"ReTAG: Reasoning Aware Table to Analytic Text Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Deepanway Ghosal,Preksha Nema,A. Raghuveer","id":"d3d8f37a809a0e3efe1cf6419033ed36424ffc8a","summary":"ReTAG is the first model that can controllably use multiple reasoning methods within a structure-aware sequence to sequence model to surpass state of the art performance in multiple table to text tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","venue":"arXiv.org","year":2023,"referenceCount":210,"citationCount":17,"influentialCitationCount":1,"publicationDate":"27/09/2023","authors":"Zheng Chu,Jingchang Chen,Qianglong Chen,Weijiang Yu,Tao He,Haotian Wang,Weihua Peng,Ming Liu,Bing Qin,Ting Liu","id":"11a4284e335ba39330b59d9f42ca3272a6166991","summary":"A thorough survey of the current research according to the taxonomies of methods within the domain of chain-of-thought reasoning, and describes XoT with frontier applications, covering planning, tool use, and distillation.","score":4},{"url":"https://www.semanticscholar.org/paper/fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":11,"influentialCitationCount":2,"publicationDate":"27/06/2023","authors":"Samy Jelassi,Stéphane d'Ascoli,Carles Domingo-Enrich,Yuhuai Wu,Yuan-Fang Li,Franccois Charton","id":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","summary":"It is shown that priming allows models trained on $5-digit $\\times$ $3-digit multiplications to generalize to $35\\times 3$ examples, and that the priming sample size scales as the logarithm of the training set size.","score":4},{"url":"https://www.semanticscholar.org/paper/d69521c1521bfa3fefb32b2106da38b1821e975d","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering","venue":"Natural Language Processing and Chinese Computing","year":2022,"referenceCount":63,"citationCount":3,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Tengxun Zhang,Hongfei Xu,Josef van Genabith,Deyi Xiong,Hongying Zan","id":"d69521c1521bfa3fefb32b2106da38b1821e975d","summary":"This paper proposes a non-autoregressive program generation framework, which independently generates complete program tuples containing both operators and operands, which can address the error propagation issue while significantly boosting the speed of program generation.","score":3},{"url":"https://www.semanticscholar.org/paper/bba47079d385bab0facf4527d5da35b7ff0a5c3d","title":"Basic Arithmetic Properties in the Space of Language Model Prompts","venue":"","year":null,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Mateusz Krubi´nski","id":"bba47079d385bab0facf4527d5da35b7ff0a5c3d","summary":"A closer look at the error distribution is taken at the performance with regard to prompt perturbations and scaling laws to better understand the performance with regard to prompt perturbations and scaling laws.","score":3},{"url":"https://www.semanticscholar.org/paper/0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":1,"influentialCitationCount":1,"publicationDate":"03/05/2023","authors":"Fengbin Zhu,Chao Wang,Fuli Feng,Zifeng Ren,Moxin Li,Tat-seng Chua","id":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","summary":"This work proposes a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements of the given question and document with Semantic-oriented hierarchical Graph structures.","score":3},{"url":"https://www.semanticscholar.org/paper/e61a96cf602ebff6683929aaf916e25614a475bc","title":"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities","venue":"arXiv.org","year":2023,"referenceCount":88,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/09/2023","authors":"Hejia Geng,Boxun Xu,Peng Li","id":"e61a96cf602ebff6683929aaf916e25614a475bc","summary":"The UPAR prompting framework is proposed, designed to emulate the structure of human cognition within LLMs, enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection and offering an epistemological foundation for existing prompting techniques.","score":3},{"url":"https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2","title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts","venue":"","year":2023,"referenceCount":89,"citationCount":21,"influentialCitationCount":2,"publicationDate":"03/10/2023","authors":"Pan Lu,Hritik Bansal,Tony Xia,Jiacheng Liu,Chun-yue Li,Hannaneh Hajishirzi,Hao Cheng,Kai-Wei Chang,Michel Galley,Jianfeng Gao","id":"8946891e94831adc8cddb0d32311cce2445c96d2","summary":"The in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning, but it still falls short of human performance by 10.4%, which underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/c22cc4e2eed78b4b31e50d94ea35da0405aabb87","title":"Multi-Operational Mathematical Derivations in Latent Space","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":"02/11/2023","authors":"Marco Valentino,Jordan Meadows,Lan Zhang,Andr'e Freitas","id":"c22cc4e2eed78b4b31e50d94ea35da0405aabb87","summary":"This paper investigates how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation.","score":3},{"url":"https://www.semanticscholar.org/paper/3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":78,"citationCount":189,"influentialCitationCount":4,"publicationDate":"31/01/2023","authors":"Simon Frieder,Luca Pinchetti,Ryan-Rhys Griffiths,Tommaso Salvatori,Thomas Lukasiewicz,P. Petersen,Alexis Chevalier,J. Berner","id":"3f2cb353c7528efafb847309ab1e1e95245740a4","summary":"It is found that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a Mathematical search engine and knowledge base interface, and GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty.","score":3},{"url":"https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":31,"citationCount":11,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Z. Chen,Kun Zhou,Beichen Zhang,Zheng Gong,Wayne Xin Zhao,Ji-rong Wen","id":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","summary":"This work proposes ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs that can effectively leverage the multi-turn conversation ability of chat- based LLMs, and integrate the thought chain following and tools manipulation in a unified way.","score":3},{"url":"https://www.semanticscholar.org/paper/87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models","venue":"arXiv.org","year":2023,"referenceCount":107,"citationCount":26,"influentialCitationCount":3,"publicationDate":"27/06/2023","authors":"Kaiyu Yang,Aidan M. Swope,Alex Gu,Rahul Chalamala,Peiyang Song,Shixing Yu,Saad Godil,R. Prenger,Anima Anandkumar","id":"87875a07976c26f82705de1fc70041169e5d652b","summary":"This paper introduces LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks, and develops ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library.","score":3},{"url":"https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf","title":"A Survey on Evaluation of Large Language Models","venue":"ACM Transactions on Intelligent Systems and Technology","year":2023,"referenceCount":302,"citationCount":146,"influentialCitationCount":5,"publicationDate":"06/07/2023","authors":"Yu-Chu Chang,Xu Wang,Jindong Wang,Yuanyi Wu,Kaijie Zhu,Hao Chen,Linyi Yang,Xiaoyuan Yi,Cunxiang Wang,Yidong Wang,Weirong Ye,Yue Zhang,Yi Chang,Philip S. Yu,Qian Yang,Xingxu Xie","id":"888728745dbb769e29ed475d4f7661eebe1a71cf","summary":"A comprehensive review of evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate, to offer invaluable insights to researchers in the realm of LLMs evaluation.","score":3},{"url":"https://www.semanticscholar.org/paper/537335d9aad0ddbaef93e7f88b0db096671ef6ec","title":"No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/09/2023","authors":"Haotian Xu","id":"537335d9aad0ddbaef93e7f88b0db096671ef6ec","summary":"A method is proposed that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning in large language models without requiring additional fine-tuning or reinforcement learning with human feedback alignment.","score":3},{"url":"https://www.semanticscholar.org/paper/e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":2,"influentialCitationCount":1,"publicationDate":"15/09/2023","authors":"Hugh Zhang,David C. Parkes","id":"e892a225c417fbac7545c3e31b45d1c42dc9c933","summary":"The central hypothesis is that chain-of-thought reasoning can act as a policy improvement operator, analogously to how Monte-Carlo Tree Search is used in AlphaZero.","score":3},{"url":"https://www.semanticscholar.org/paper/8d806a91e5f2166ee6823eb7e6e8e56826b6776d","title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":2,"influentialCitationCount":0,"publicationDate":"27/09/2023","authors":"Linxin Song,Jieyu Zhang,Lechao Cheng,Pengyuan Zhou,Tianyi Zhou,Irene Li","id":"8d806a91e5f2166ee6823eb7e6e8e56826b6776d","summary":"This study presents a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams, and reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance.","score":3},{"url":"https://www.semanticscholar.org/paper/b16c7d45183b9d595ab64301be019741b1528860","title":"Llemma: An Open Language Model For Mathematics","venue":"arXiv.org","year":2023,"referenceCount":92,"citationCount":22,"influentialCitationCount":8,"publicationDate":"16/10/2023","authors":"Zhangir Azerbayev,Hailey Schoelkopf,Keiran Paster,Marco Dos Santos,Stephen McAleer,Albert Q. Jiang,Jia Deng,Stella Biderman,S. Welleck","id":"b16c7d45183b9d595ab64301be019741b1528860","summary":"Llemma is a large language model for mathematics that outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis, and is capable of tool use and formal theorem proving without any further finetuning.","score":3},{"url":"https://www.semanticscholar.org/paper/44b506d9619b5f957dc2b5588801138f343c0308","title":"Let's reward step by step: Step-Level reward model as the Navigators for Reasoning","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":2,"influentialCitationCount":1,"publicationDate":"16/10/2023","authors":"Qianli Ma,Haotian Zhou,Tingkai Liu,Jianbo Yuan,Pengfei Liu,Yang You,Hongxia Yang","id":"44b506d9619b5f957dc2b5588801138f343c0308","summary":"This work proposes a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs and demonstrates enhanced results compared to the Chain of Thought on mathematical benchmarks like GSM8K and MATH.","score":3},{"url":"https://www.semanticscholar.org/paper/df15b83986207d884a811ce572dcedb6654abc6f","title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2023","authors":"Rik Koncel-Kedziorski,Michael Krumdick,V. Lai,Varshini Reddy,Charles Lovering,Chris Tanner","id":"df15b83986207d884a811ce572dcedb6654abc6f","summary":"An in-depth evaluation of open-source and commercial LLMs is conducted, illustrating that BizBench is a challenging benchmark for quantitative reasoning in the finance and business domain.","score":3},{"url":"https://www.semanticscholar.org/paper/7017c58e19f4db0c38040935cc9fb7b7090a466d","title":"Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2023","authors":"Haoran Liao,Qinyi Du,Shaohua Hu,Hao He,Yanyan Xu,Jidong Tian,Yaohui Jin","id":"7017c58e19f4db0c38040935cc9fb7b7090a466d","summary":"This work proposes a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named PRER and provides and implements two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations.","score":3},{"url":"https://www.semanticscholar.org/paper/608a2b333fd8262e8c918f36c5700bafd3ea3cdd","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2023","authors":"Mehran Kazemi,Hamidreza Alvari,Ankit Anand,Jialin Wu,Xi Chen,Radu Soricut","id":"608a2b333fd8262e8c918f36c5700bafd3ea3cdd","summary":"The empirical results obtained using the benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/675629ff78cef09665a1135fece66195ed80a640","title":"MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline","venue":"arXiv.org","year":2024,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/01/2024","authors":"Minpeng Liao,Wei Luo,Chengxi Li,Jing Wu,Kai Fan","id":"675629ff78cef09665a1135fece66195ed80a640","summary":"This paper introduces a novel math dataset, enhanced with a capability to utilize a Python code interpreter, and proposes a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets.","score":3},{"url":"https://www.semanticscholar.org/paper/9c6cecf409ca6257e717291b47c2cf8b75cf4a58","title":"Learning Multi-step Reasoning from Arithmetic Task","venue":"","year":2023,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Tianduo Wang,Wei Lu","id":"9c6cecf409ca6257e717291b47c2cf8b75cf4a58","summary":"This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning by continually pre-training LMs on a synthetic dataset M S AT, which stands for M ulti- s tep A rithmetic T ask.","score":3},{"url":"https://www.semanticscholar.org/paper/668701cdb6f4f5a079ad60173885c172dde20f17","title":"Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving","venue":"Information Processing & Management","year":2024,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2024","authors":"Yi Zhang,Guangyou Zhou,Zhiwen Xie,Xiangji Huang","id":"668701cdb6f4f5a079ad60173885c172dde20f17","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/5938abafd61881f6b23a2ba318d2d3d0327402c0","title":"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram","venue":"International Joint Conference on Artificial Intelligence","year":2023,"referenceCount":46,"citationCount":8,"influentialCitationCount":1,"publicationDate":"22/02/2023","authors":"Ming-Liang Zhang,Fei Yin,Cheng-Lin Liu","id":"5938abafd61881f6b23a2ba318d2d3d0327402c0","summary":"This work converts diagrams into basic textual clauses to describe diagram features effectively, and proposes a new neural solver called PGPSNet to fuse multi-modal information efficiently and promote geometric understanding and reasoning.","score":3},{"url":"https://www.semanticscholar.org/paper/c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":417,"citationCount":726,"influentialCitationCount":54,"publicationDate":"31/03/2023","authors":"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen","id":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","summary":"A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","score":3},{"url":"https://www.semanticscholar.org/paper/bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks","venue":"","year":2023,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/04/2023","authors":"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao","id":"bebede105aa69a81045bf79682272ee4cfb61475","summary":"By incorporating CoNN modules into the LM, the framework effectively tackles rule-intensive challenges and achieves flawless execution on symbolic operations tasks, highlighting the potential of the method in enabling LMs to possess true symbolic comprehension capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":51,"influentialCitationCount":8,"publicationDate":"19/04/2023","authors":"Chuanyang Zheng,Zhengying Liu,Enze Xie,Zhenguo Li,Yu Li","id":"261549439aebdda72b648ecc462448fd24857ac1","summary":"This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers.","score":3},{"url":"https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":53,"citationCount":77,"influentialCitationCount":9,"publicationDate":"06/05/2023","authors":"Lei Wang,Wanyu Xu,Yihuai Lan,Zhiqiang Hu,Yunshi Lan,R. Lee,Ee-Peng Lim","id":"62176de125738e3b95850d1227bac81fd646b78e","summary":"The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.","score":3},{"url":"https://www.semanticscholar.org/paper/a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/05/2023","authors":"Qianying Liu,Dongsheng Yang,Wenjie Zhong,Fei Cheng,S. Kurohashi","id":"a39f960b28a627d554712a457efa5d3e3b7f8406","summary":"Three pretraining tasks that operate at both the whole program and sub-program level are proposed: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the modelto identify key evidence that sub- programs are derived from.","score":3},{"url":"https://www.semanticscholar.org/paper/4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":5,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Jordan Meadows,Marco Valentino,Damien Teney,André Freitas","id":"4313b009cd30a469400d36b9fb20267620293d04","summary":"A data generation method for producing intricate mathematical derivations, and systematically perturb them with respect to syntax, structure, and semantics, employing symbolic algebra for scalable data production and augmentation.","score":3},{"url":"https://www.semanticscholar.org/paper/8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":14,"influentialCitationCount":0,"publicationDate":"02/06/2023","authors":"Yiran Wu,Feiran Jia,Shaokun Zhang,Han-Tai Li,Erkang Zhu,Yue Wang,Y. Lee,Richard Peng,Qingyun Wu,Chi Wang","id":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","summary":"This work explores the frontier of using GPT-4 for solving more complex and challenging math problems, and considers MathChat, a conversational problem-solving framework newly proposed in this work.","score":3},{"url":"https://www.semanticscholar.org/paper/0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements","venue":"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)","year":2023,"referenceCount":111,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/06/2023","authors":"Syed Rifat Raiyan,Md. Nafis Faiyaz,S. Kabir,Mohsinul Kabir,H. Mahmud,Md. Kamrul Hasan","id":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","summary":"This paper proposes a framework for MWP solvers based on the generation of linguistic variants of the problem text and shows that training on linguistic variant of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model.","score":3},{"url":"https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":447,"citationCount":39,"influentialCitationCount":1,"publicationDate":"12/07/2023","authors":"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,N. Barnes,A. Mian","id":"ca31b8584b6c022ef15ddfe994fe361e002b7729","summary":"A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","score":3},{"url":"https://www.semanticscholar.org/paper/412fe1f135cb20c952962133ca1e534a71bfd27f","title":"When Do Program-of-Thoughts Work for Reasoning?","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":9,"influentialCitationCount":2,"publicationDate":"29/08/2023","authors":"Zhen Bi,Ningyu Zhang,Yinuo Jiang,Shumin Deng,Guozhou Zheng,Huajun Chen","id":"412fe1f135cb20c952962133ca1e534a71bfd27f","summary":"This work uses the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity, and designs an auto-synthesizing and stratifying algorithm, which is applied to instruction generation for mathematical reasoning and code data filtering for code generation tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/8db1dcae055842f43ccac04182957b20d15bbe6b","title":"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":3,"influentialCitationCount":0,"publicationDate":"03/10/2023","authors":"Aniruddha Deb,Neeva Oza,Sarthak Singla,Dinesh Khandelwal,Dinesh Garg,Parag Singla","id":"8db1dcae055842f43ccac04182957b20d15bbe6b","summary":"This paper formally defines the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith, and proposes three novel techniques that improve performance.","score":3},{"url":"https://www.semanticscholar.org/paper/cddb552f6c3464a54a02b0b64b2d1af56c086606","title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":5,"influentialCitationCount":0,"publicationDate":"05/10/2023","authors":"Ke Wang,Houxing Ren,Aojun Zhou,Zimu Lu,Sichun Luo,Weikang Shi,Renrui Zhang,Linqi Song,Mingjie Zhan,Hongsheng Li","id":"cddb552f6c3464a54a02b0b64b2d1af56c086606","summary":"This paper proposes a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities, and yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.","score":3},{"url":"https://www.semanticscholar.org/paper/98b607e7cb84e1a5c87c8a49562ae35435e6722d","title":"Learning From Mistakes Makes LLM Better Reasoner","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"31/10/2023","authors":"Shengnan An,Zexiong Ma,Zeqi Lin,Nanning Zheng,Jian-Guang Lou,Weizhu Chen","id":"98b607e7cb84e1a5c87c8a49562ae35435e6722d","summary":"LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4 and improves the performance compared with fine-tuning on CoT data alone, surpassing the SOTA performance achieved by non-execution open-source models on these challenging tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/44ffd3cd333bb41b900926556edb1c452759c398","title":"WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/11/2023","authors":"Youssef Benchekroun,Megi Dervishi,Mark Ibrahim,Jean-Baptiste Gaya,Xavier Martinet,Grégoire Mialon,Thomas Scialom,Emmanuel Dupoux,D. Hupkes,Pascal Vincent","id":"44ffd3cd333bb41b900926556edb1c452759c398","summary":"WorldSense is a benchmark designed to assess the extent to which LLMs are consistently able to sustain tacit world models, by testing how they draw simple inferences from descriptions of simple arrangements of entities by using Worldsense, a synthetic benchmark with three problem types.","score":3},{"url":"https://www.semanticscholar.org/paper/af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning","venue":"arXiv.org","year":2024,"referenceCount":179,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2024","authors":"Yiqi Wang,Wentao Chen,Xiaotian Han,Xudong Lin,Haiteng Zhao,Yongfei Liu,Bohan Zhai,Jianbo Yuan,Quanzeng You,Hongxia Yang","id":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","summary":"This survey comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions.","score":3},{"url":"https://www.semanticscholar.org/paper/59084df7203c6be33838ba3e3854eb9bda053ed2","title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint","venue":"arXiv.org","year":2024,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/01/2024","authors":"Zhipeng Chen,Kun Zhou,Wayne Xin Zhao,Junchen Wan,Fuzheng Zhang,Di Zhang,Ji-Rong Wen","id":"59084df7203c6be33838ba3e3854eb9bda053ed2","summary":"A new RL method named RLMEC is proposed that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training.","score":3},{"url":"https://www.semanticscholar.org/paper/c6e162aedf6a5ab0135e3b991577d77ca06673f9","title":"SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning","venue":"arXiv.org","year":2024,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/01/2024","authors":"Dan Zhang,Ziniu Hu,Sining Zhoubian,Zhengxiao Du,Kaiyu Yang,Zihan Wang,Yisong Yue,Yuxiao Dong,Jie Tang","id":"c6e162aedf6a5ab0135e3b991577d77ca06673f9","summary":"SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning, is introduced, with a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain.","score":3},{"url":"https://www.semanticscholar.org/paper/2c6896c025b29a2cc3d90bbf9b77646b255b2090","title":"TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance","venue":"","year":2024,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2024","authors":"Haorui Wang,Rongzhi Zhang,Yinghao Li,Lingkai Kong,Yuchen Zhuang,Xiusi Chen,Chao Zhang College of Computing,G. I. O. Technology,Department of Computer Science,U. California,Los Angeles","id":"2c6896c025b29a2cc3d90bbf9b77646b255b2090","summary":"This work introduces a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD), which mimics the interaction between a teacher and a student using a principle-based approach.","score":3},{"url":"https://www.semanticscholar.org/paper/0231b9300bdc0cdf10ea0f5c48d3a210246b87c0","title":"Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts","venue":"","year":2024,"referenceCount":231,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2024","authors":"Maciej Besta,Florim Memedi,Zhenyu Zhang,Robert Gerstenberger,Nils Blach,Piotr Nyczyk,Marcin Copik,Grzegorz Kwa'sniewski,Jurgen Muller,Lukas Gianinazzi,Aleš Kubíček,H. Niewiadomski,Onur Mutlu,Torsten Hoefler","id":"0231b9300bdc0cdf10ea0f5c48d3a210246b87c0","summary":"This work conducts an in-depth analysis of the prompt execution pipeline, and builds the first taxonomy of structure-enhanced LLM reasoning schemes, referring to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context.","score":3},{"url":"https://www.semanticscholar.org/paper/d9e76ae6480114d81da2e9eb98f848df120be057","title":"S ALSA F RESCA : Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors","venue":"","year":null,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Samuel Stevens,Emily Wenger,Cathy Li,Niklas Nolte,Eshika Saxena,François Charton,Kristin Lauter","id":"d9e76ae6480114d81da2e9eb98f848df120be057","summary":"This work is the first instance of ML attacks recovering sparse binary secrets in dimension n = 1024, the smallest dimension used in practice for homomorphic encryption applications of LWE where sparse binary secrets are proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","venue":"Conference and Labs of the Evaluation Forum","year":2022,"referenceCount":58,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Wei Zhong,Yuqing Xie,Jimmy J. Lin","id":"c27f3904490b8e5f9f39fe2b36722090c189e916","summary":"This work describes the participation of the team in the ARQMath 2022 Lab, where two highly complementary methods for effective math answer and formula retrieval are applied, using a lexical sparse retriever and a fine-tuned bi-encoder dense retriever to capture contextual similarity and semantic matching.","score":3},{"url":"https://www.semanticscholar.org/paper/1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)","venue":"arXiv.org","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Victor Robila,Kexin Pei,Junfeng Yang","id":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","summary":"A comparison between human-developed machine learning model and models sampled through Neural Architecture Search (NAS) determine an efficient approach to solve the problem of addition using embedded hexadecimal digits.","score":3},{"url":"https://www.semanticscholar.org/paper/0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Vaishali Pal,Andrew Yates,E. Kanoulas,M. de Rijke","id":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","summary":"This model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers, which outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.","score":3},{"url":"https://www.semanticscholar.org/paper/d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling","venue":"arXiv.org","year":2023,"referenceCount":107,"citationCount":9,"influentialCitationCount":0,"publicationDate":"01/06/2023","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","summary":"This work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning, and hypothesizes that attention glitches account for some of the closed-domain hallucinations in natural LLMs.","score":3},{"url":"https://www.semanticscholar.org/paper/edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","title":"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/08/2023","authors":"Xingcheng Xu,Zihao Pan,Haipeng Zhang,Yanqing Yang","id":"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","summary":"It is discovered that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures.","score":3},{"url":"https://www.semanticscholar.org/paper/cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","title":"Can transformers learn the greatest common divisor?","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/08/2023","authors":"Franccois Charton","id":"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/09/2023","authors":"Dimitris Spathis,F. Kawsar","id":"0b778079946764292de3771a489d5ce9e1868a8b","summary":"Recent works that employ LLMs for human-centric tasks are discussed and a case study showing that popular LLMs tokenize temporal data incorrectly is presented, highlighting potential solutions that can help bridge this \"modality gap\".","score":3},{"url":"https://www.semanticscholar.org/paper/1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","title":"What Algorithms can Transformers Learn? A Study in Length Generalization","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/10/2023","authors":"Hattie Zhou,Arwen Bradley,Etai Littwin,Noam Razin,O. Saremi,Josh Susskind,Samy Bengio,Preetum Nakkiran","id":"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","summary":"This work proposes a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task and provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.","score":3},{"url":"https://www.semanticscholar.org/paper/2aeaad5548229dec7fdf716f7e83a5a359665852","title":"Carrying over algorithm in transformers","venue":"arXiv.org","year":2024,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/01/2024","authors":"J. Kruthoff","id":"2aeaad5548229dec7fdf716f7e83a5a359665852","summary":"A simple way of precisely identifying which neurons are responsible for that task in the carrying over algorithm is provided, across a range of hyperparameters for two as well as three-layer models.","score":3},{"url":"https://www.semanticscholar.org/paper/34ca51ce10e8d1d6c950ba519329714a0184d004","title":"KwaiYiiMath: Technical Report","venue":"arXiv.org","year":2023,"referenceCount":123,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2023","authors":"Jia-Yi Fu,Lei Lin,Xiaoyang Gao,Pengli Liu,Zhengzong Chen,Zhirui Yang,Shengnan Zhang,Xue Zheng,Yan Li,Yuliang Liu,Xucheng Ye,Yiqiao Liao,Chao Liao,Bin Chen,Chengru Song,Junchen Wan,Zijia Lin,Fuzheng Zhang,Zhongyuan Wang,Di Zhang,Kun Gai","id":"34ca51ce10e8d1d6c950ba519329714a0184d004","summary":"The KwaiyiiMath is introduced, which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks","venue":"Neural Information Processing Systems","year":2022,"referenceCount":181,"citationCount":39,"influentialCitationCount":4,"publicationDate":"14/06/2022","authors":"Tuan Dinh,Yuchen Zeng,Ruisu Zhang,Ziqian Lin,Shashank Rajput,Michael Gira,Jy-yong Sohn,Dimitris Papailiopoulos,Kangwook Lee","id":"b09420b30fc093d63fb2ee1aac26c71da81da437","summary":"Language-Interfaced Fine-Tuning is proposed and found that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata","venue":"International Conference on Learning Representations","year":2022,"referenceCount":107,"citationCount":52,"influentialCitationCount":3,"publicationDate":"19/10/2022","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"e82e3f4347674b75c432cb80604d38ee630d4bf6","summary":"It is found that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics.","score":3},{"url":"https://www.semanticscholar.org/paper/e3aa232577bb427b1f3a34acbdef84bd85734042","title":"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":17,"influentialCitationCount":3,"publicationDate":"30/08/2023","authors":"Chi Han,Qifan Wang,Wenhan Xiong,Yu Chen,Heng Ji,Sinong Wang","id":"e3aa232577bb427b1f3a34acbdef84bd85734042","summary":"LM-Infinite is computationally efficient with $O(n)$ time and space, and demonstrates consistent text generation fluency and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup.","score":3},{"url":"https://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence","venue":"NAACL-HLT","year":2022,"referenceCount":53,"citationCount":5,"influentialCitationCount":1,"publicationDate":"08/05/2022","authors":"Myeongjun Jang,Frank Mtumbuka,Thomas Lukasiewicz","id":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","summary":"A novel intermediate training task, names meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":54,"citationCount":4,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski","id":"0cf5c61f367e3937dc07be634a43952a50b589a4","summary":"This work introduces an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test, and demonstrates how the framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.","score":3},{"url":"https://www.semanticscholar.org/paper/6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":45,"citationCount":6,"influentialCitationCount":3,"publicationDate":"23/11/2022","authors":"Xiao Li,Yin Zhu,Sichen Liu,Jiangzhou Ju,Yuzhong Qu,Gong Cheng","id":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","summary":"DyRRen is proposed, an extended retriever-reranker-generator framework where each generation step is enhanced by a dynamic reranking of retrieved sentences that outperforms existing baselines on the FinQA dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts","venue":"Expert systems with applications","year":2023,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/04/2023","authors":"Donghee Choi,Mogan Gim,Samy Badreddine,Hajung Kim,Donghyeon Park,Jaewoo Kang","id":"be8f769f135b0ffcf829594b9421788781f38008","summary":"This work introduces KitchenScale, a fine-tuned Pre- trained Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context and adopted the Discrete Latent Exponent method to cope with high variance of numerical scales in recipe corpora.","score":3},{"url":"https://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":71,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Pratyay Banerjee,Swaroop Mishra","id":"884c0b6db564208d99cadf2548f0aa96dee5f859","summary":"This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora to develop unstructured commonsense knowledge sources and explores three strategies for knowledge incorporation.","score":3},{"url":"https://www.semanticscholar.org/paper/f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","summary":"Investigation of the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers finds that two layer neural networks fail to learn the structure of this task and that growing the network’s width leads to a complex division of input space.","score":3},{"url":"https://www.semanticscholar.org/paper/d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":132,"citationCount":34,"influentialCitationCount":1,"publicationDate":"12/04/2021","authors":"Tara Safavi,Danai Koutra","id":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","summary":"This work proposes to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision, and provides a high-level, extensible taxonomy for knowledge representation in L Ms.","score":3},{"url":"https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding","venue":"International Conference on Learning Representations","year":2022,"referenceCount":93,"citationCount":12,"influentialCitationCount":2,"publicationDate":"06/04/2022","authors":"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah","id":"706c6b3781374b0b11f98f204a4ddd05b26ed009","summary":"Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":43,"citationCount":14,"influentialCitationCount":1,"publicationDate":"20/04/2021","authors":"Lisa Bauer","id":"d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a","summary":"This work presents an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which is called KG-to-task match and demonstrates that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the bestmatch for PIQA across all 3 analysis phases.","score":3},{"url":"https://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":84,"citationCount":33,"influentialCitationCount":0,"publicationDate":"28/01/2022","authors":"Prajjwal Bhargava,Vincent Ng","id":"7e5ca499cd9b932921bda84db98f75087d0b0683","summary":"A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.","score":3},{"url":"https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":46,"citationCount":9,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Himanshu Gupta,Neeraj Varshney,Swaroop Mishra,Kuntal Kumar Pal,Saurabh Arjun Sawant,Kevin Scaria,Siddharth Goyal,Chitta Baral","id":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","summary":"This work introduces FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility, and shows that even state-of-the-art models such as GPT-3, G PT-2, and T5 struggle to answer the feasibility questions correctly.","score":3},{"url":"https://www.semanticscholar.org/paper/23cc6b2ed88872fcd3767cf054100e8eddcdb0a1","title":"CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2023","authors":"Mete Ismayilzada,Debjit Paul,Syrielle Montariol,Mor Geva,Antoine Bosselut","id":"23cc6b2ed88872fcd3767cf054100e8eddcdb0a1","summary":"A significant performance gap is found when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world task settings.","score":3},{"url":"https://www.semanticscholar.org/paper/b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":57,"citationCount":42,"influentialCitationCount":6,"publicationDate":"24/05/2021","authors":"Shunyu Yao,Binghui Peng,C. Papadimitriou,Karthik Narasimhan","id":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","summary":"It is proved that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.","score":3},{"url":"https://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","venue":"International Conference on Learning Representations","year":2021,"referenceCount":48,"citationCount":280,"influentialCitationCount":52,"publicationDate":"27/08/2021","authors":"Ofir Press,Noah A. Smith,M. Lewis","id":"9ca329408813d209b1dcb36936f7f9cba82506bd","summary":"This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).","score":3},{"url":"https://www.semanticscholar.org/paper/dc48bc1a4d81e0f37603013fd2a95644dc233bd0","title":"Functional Interpolation for Relative Positions Improves Long Context Transformers","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":4,"influentialCitationCount":1,"publicationDate":"06/10/2023","authors":"Shanda Li,Chong You,Guru Guruganesh,J. Ainslie,Santiago Ontanon,M. Zaheer,Sumit K. Sanghai,Yiming Yang,Sanjiv Kumar,Srinadh Bhojanapalli","id":"dc48bc1a4d81e0f37603013fd2a95644dc233bd0","summary":"It is theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple, and empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/42b877f7423fc727bff5b6e173ad336da33079a9","title":"Uncovering hidden geometry in Transformers via disentangling position and context","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":2,"influentialCitationCount":1,"publicationDate":"07/10/2023","authors":"Jiajun Song,Yiqiao Zhong","id":"42b877f7423fc727bff5b6e173ad336da33079a9","summary":"This decomposition of hidden states (or embeddings) of trained transformers into interpretable components offers structural insights about input formats in in-context learning (especially for induction heads) and in arithmetic tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/200fcd964f41efe0c35a3f888a520ede08a3269c","title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/10/2023","authors":"Shaoxiong Duan,Yining Shi","id":"200fcd964f41efe0c35a3f888a520ede08a3269c","summary":"This paper investigates the inherent capabilities of transformer models in learning arithmetic algorithms and introduces Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which are linked to mechanisms in relative position encoding.","score":3},{"url":"https://www.semanticscholar.org/paper/14336fbb221da89d77b1e54f1d477c0a8cb0ef85","title":"LangBridge: Multilingual Reasoning Without Multilingual Supervision","venue":"arXiv.org","year":2024,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2024","authors":"Dongkeun Yoon,Joel Jang,Sungdong Kim,Seungone Kim,Sheikh Shafayat,Minjoon Seo","id":"14336fbb221da89d77b1e54f1d477c0a8cb0ef85","summary":"Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning.","score":3},{"url":"https://www.semanticscholar.org/paper/f5df0667365764a970fc6abfa0a68b7d1d0ae413","title":"Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models","venue":"arXiv.org","year":2024,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/01/2024","authors":"Asma Ghandeharioun,Avi Caciularu,Adam Pearce,Lucas Dixon,Mor Geva","id":"f5df0667365764a970fc6abfa0a68b7d1d0ae413","summary":"It is shown that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework, and several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes.","score":2},{"url":"https://www.semanticscholar.org/paper/69bfa665e507fcee4a8d003933998eb89f336c9f","title":"Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning","venue":"","year":null,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Yingcong Li,Kartik K. Sreenivasan,Angeliki Giannou,Dimitris Papailiopoulos,Samet Oymak","id":"69bfa665e507fcee4a8d003933998eb89f336c9f","summary":"This study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs).","score":2},{"url":"https://www.semanticscholar.org/paper/123acfbccca0460171b6b06a4012dbb991cde55b","title":"Large Language Models Are Zero-Shot Time Series Forecasters","venue":"arXiv.org","year":2023,"referenceCount":77,"citationCount":15,"influentialCitationCount":3,"publicationDate":"11/10/2023","authors":"Nate Gruver,Marc Finzi,Shikai Qiu,Andrew Gordon Wilson","id":"123acfbccca0460171b6b06a4012dbb991cde55b","summary":"This work proposes procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values and shows how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions.","score":2},{"url":"https://www.semanticscholar.org/paper/4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":45,"citationCount":19,"influentialCitationCount":3,"publicationDate":"15/09/2021","authors":"Zhoujun Cheng,Haoyu Dong,Fan Cheng,Ran Jia,Pengfei Wu,Shi Han,Dongmei Zhang","id":"4ef578869c957f5fb969fa9c164dca0433f48042","summary":"This paper proposes FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining with tree attention, and proposes two novel self-supervised pretraining objectives derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP).","score":2},{"url":"https://www.semanticscholar.org/paper/cbccb1201eee6432020276762a44ebfbf8f981a0","title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge","venue":"arXiv.org","year":2022,"referenceCount":40,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo","id":"cbccb1201eee6432020276762a44ebfbf8f981a0","summary":"A novel unsupervised methodology leveraging external knowledge and contextualized word embeddings from ClinicalBERT for numerical reasoning in a variety of phenotypic contexts is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/09/2023","authors":"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu","id":"3ec464696db25acc2c39a6d967ec3df09e06f633","summary":"A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","score":2},{"url":"https://www.semanticscholar.org/paper/efcd9c1438559d908efd702333232078fd251a0f","title":"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification","venue":"Experimental biology and medicine","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo","id":"efcd9c1438559d908efd702333232078fd251a0f","summary":"This article proposes a novel unsupervised method that leverages external clinical knowledge and contextualized word embeddings by ClinicalBERT for numerical reasoning in different phenotypic contexts and finds that these phenotypes from clinical text can be used to impute the missing values in structured data, which enrich and improve data quality.","score":2},{"url":"https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":40,"citationCount":76,"influentialCitationCount":11,"publicationDate":"20/12/2022","authors":"Boshi Wang,Sewon Min,Xiang Deng,Jiaming Shen,You Wu,Luke Zettlemoyer,Huan Sun","id":"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","summary":"It is shown that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.","score":2},{"url":"https://www.semanticscholar.org/paper/27f32dc1aab7919eb7039deca067c3fbdc719c2a","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"26/03/2023","authors":"Thuy-Trang Vu,Xuanli He,Gholamreza Haffari,Ehsan Shareghi","id":"27f32dc1aab7919eb7039deca067c3fbdc719c2a","summary":"Koala, a searchable index over large pre-training corpora using compressed suffix arrays with highly efficient compression rate and search support, is launched, providing a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs.","score":2},{"url":"https://www.semanticscholar.org/paper/a563fa042b16533d23829d76e7e17bf19a05891c","title":"Large Language Models Are Not Strong Abstract Reasoners","venue":"","year":2023,"referenceCount":66,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/05/2023","authors":"Gaël Gendron,Qiming Bao,M. Witbrock,G. Dobbie","id":"a563fa042b16533d23829d76e7e17bf19a05891c","summary":"It is argued that guiding LLM generation to follow causal paths could help improve the generalisation and reasoning abilities of LLMs, and currently achieve very limited performance in contrast with other natural language tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/6a3c67d35f3a9c10c8fde6c325a0535c03876068","title":"SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2023","authors":"Jonathan Tonglet,Manon Reusens,Philipp Borchert,Bart Baesens","id":"6a3c67d35f3a9c10c8fde6c325a0535c03876068","summary":"This work presents Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse and formulates exemplar selection as a Knapsack Integer Linear Program.","score":2},{"url":"https://www.semanticscholar.org/paper/800bb1ad43778ed7682e21ffbf9b2d12ffdd08c9","title":"Visualization Generation with Large Language Models: An Evaluation","venue":"","year":2024,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2024","authors":"Guozheng Li,Xinyu Wang,Gerile Aodeng,Shunyuan Zheng,Yu Zhang,Chuangxin Ou,Song Wang,Chi Harold Liu","id":"800bb1ad43778ed7682e21ffbf9b2d12ffdd08c9","summary":"This paper evaluates the capability of a large language model to generate visualization specifications on the task of natural language to visualization (NL2VIS), and chooses GPT-3.5 and Vega-Lite to represent large language models and visualization specifications, respectively.","score":2},{"url":"https://www.semanticscholar.org/paper/7e4f5589327b6b574cc950a03fd1d6236e9e6128","title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":67,"citationCount":38,"influentialCitationCount":2,"publicationDate":"25/04/2023","authors":"Ori Yoran,Tomer Wolfson,Ben Bogin,Uri Katz,Daniel Deutch,Jonathan Berant","id":"7e4f5589327b6b574cc950a03fd1d6236e9e6128","summary":"This work introduces Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers.","score":2},{"url":"https://www.semanticscholar.org/paper/7eb044170c11b7e2193b8df35f606edcfc7f2585","title":"Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/05/2023","authors":"Xiang Zhang,Senyu Li,B. Hauer,Ning Shi,Grzegorz Kondrak","id":"7eb044170c11b7e2193b8df35f606edcfc7f2585","summary":"This work investigates the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingsual capabilities, and proposes a systematic way of qualifying the performance disparities of LLMs under multilingual settings.","score":2},{"url":"https://www.semanticscholar.org/paper/ca4d1d9c61e6496bbb0f361b55413f265d81de6e","title":"Prompting Large Language Models for Malicious Webpage Detection","venue":"2023 IEEE 4th International Conference on Pattern Recognition and Machine Learning (PRML)","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/08/2023","authors":"Lu Li,Bojie Gong","id":"ca4d1d9c61e6496bbb0f361b55413f265d81de6e","summary":"This work proposes a novel approach for malicious webpage detection by leveraging Large Language Models (LLMs), and studies zero-shot and few-shot prompting methods to adapt those LLMs to perform malicious webpage detection.","score":2},{"url":"https://www.semanticscholar.org/paper/692902404c282de936249aef68ce9f974815128c","title":"CLEVA: Chinese Language Models EVAluation Platform","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":95,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/08/2023","authors":"Yanyang Li,Jianqiao Zhao,Duo Zheng,Zi-Yuan Hu,Zhi Chen,Xiaohui Su,Yongfeng Huang,Shijia Huang,Dahua Lin,Michael R. Lyu,Liwei Wang","id":"692902404c282de936249aef68ce9f974815128c","summary":"CLEVA is a user-friendly platform crafted to holistically evaluate Chinese LLMs, employing a standardized workflow to assess LLMs' performance across various dimensions, regularly updating a competitive leaderboard.","score":2},{"url":"https://www.semanticscholar.org/paper/287a30043ad1c3e349095af7e3e42d3be3b6c0c9","title":"SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/10/2023","authors":"Kazem Meidani,Parshin Shojaee,Chandan K. Reddy,A. Farimani","id":"287a30043ad1c3e349095af7e3e42d3be3b6c0c9","summary":"SNIP, a Symbolic-Numeric Integrated Pre-training, is introduced, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings, and provides cross-domain insights into the representations.","score":2},{"url":"https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","venue":"arXiv.org","year":2023,"referenceCount":186,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/11/2023","authors":"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong","id":"8d2709ed1788a67e64425fb410bb49f3ee49e088","summary":"This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","score":2},{"url":"https://www.semanticscholar.org/paper/89ed7fd00319d45269906a9b05e10c8680bf9cec","title":"FinanceBench: A New Benchmark for Financial Question Answering","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2023","authors":"Pranab Islam,Anand Kannappan,Douwe Kiela,Rebecca Qian,Nino Scherrer,Bertie Vidgen","id":"89ed7fd00319d45269906a9b05e10c8680bf9cec","summary":"It is shown that existing LLMs have clear limitations for financial QA, and all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.","score":2},{"url":"https://www.semanticscholar.org/paper/e99de66608a3b060d54548b9e9a7c39961872cd7","title":"LANS: A Layout-Aware Neural Solver for Plane Geometry Problem","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/11/2023","authors":"Ming-Liang Zhang,Zhong-Zhi Li,Fei Yin,Cheng-Lin Liu","id":"e99de66608a3b060d54548b9e9a7c39961872cd7","summary":"Extensive experiments on datasets Geometry3K and PGPS9K validate the effectiveness of the layout-aware modules and superior problem solving performance of the LANS solver, over existing symbolic solvers and neural solvers.","score":2},{"url":"https://www.semanticscholar.org/paper/26ffac7935a78328ef6070d3b890d201dc378ef1","title":"SUFFI-GPSC: Sufficient Geometry Problem Solution Checking with Symbolic Computation and Logical Reasoning","venue":"2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2023","authors":"Yulong Yang,Hongguang Fu,Xiuqin Zhong,Tianming Zhang","id":"26ffac7935a78328ef6070d3b890d201dc378ef1","summary":"This work proposes a powerful approach that can check the logical sufficiency of the solving process of given geometry problems step by step with symbolic computation and logical reasoning, called Sufficient Geometry problem solution checking (Suffi-GPSC).","score":2},{"url":"https://www.semanticscholar.org/paper/cc8417aa578016203cb52efc63592bba64b08bb3","title":"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":35,"citationCount":9,"influentialCitationCount":3,"publicationDate":2022,"authors":"Chenying Li,Wenbo Ye,Yilun Zhao","id":"cc8417aa578016203cb52efc63592bba64b08bb3","summary":"A new framework named FinMath is proposed, which improves the model’s numerical reasoning capacity by injecting a tree-structured neural model to perform multi-step numerical reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","venue":"BIGSCIENCE","year":2022,"referenceCount":142,"citationCount":437,"influentialCitationCount":47,"publicationDate":"14/04/2022","authors":"Sid Black,Stella Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,USVSN Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach","id":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","summary":"GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","score":2},{"url":"https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models","venue":"Neural Information Processing Systems","year":2022,"referenceCount":33,"citationCount":80,"influentialCitationCount":16,"publicationDate":"11/07/2022","authors":"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur","id":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","summary":"This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.","score":2},{"url":"https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science","venue":"arXiv.org","year":2022,"referenceCount":107,"citationCount":336,"influentialCitationCount":49,"publicationDate":"16/11/2022","authors":"Ross Taylor,Marcin Kardas,Guillem Cucurull,Thomas Scialom,A. Hartshorn,Elvis Saravia,Andrew Poulton,Viktor Kerkez,Robert Stojnic","id":"7d645a3fd276918374fd9483fd675c28e46506d1","summary":"Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.","score":2},{"url":"https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety","venue":"arXiv.org","year":2022,"referenceCount":43,"citationCount":9,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Joshua Albrecht,Ellie Kitanidis,Abraham J. Fetterman","id":"7d5175db1b99552491063d2d9581b0b51e1d2932","summary":"This work provides a simple new prompting strategy that leads to yet another supposedly \"super-human\" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset).","score":2},{"url":"https://www.semanticscholar.org/paper/033275ccc2c7c5c38592ae893da0b5923cf90717","title":"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases","venue":"arXiv.org","year":2023,"referenceCount":171,"citationCount":18,"influentialCitationCount":1,"publicationDate":"26/03/2023","authors":"Hongyu Ren,Mikhail Galkin,Michael Cochez,Zhaocheng Zhu,J. Leskovec","id":"033275ccc2c7c5c38592ae893da0b5923cf90717","summary":"A holistic survey of CLQA is provided with a detailed taxonomy studying the field from multiple angles, including graph types (modality, reasoning domain, background semantics), modeling aspects, modeling aspects (encoder, processor, decoder), supported queries, datasets, evaluation metrics, and applications.","score":2},{"url":"https://www.semanticscholar.org/paper/8236010c2ecc94d826be6010ff187fdc000e7df6","title":"Deductive Verification of Chain-of-Thought Reasoning","venue":"arXiv.org","year":2023,"referenceCount":70,"citationCount":31,"influentialCitationCount":2,"publicationDate":"06/06/2023","authors":"Z. Ling,Yunhao Fang,Xuanlin Li,Zhiao Huang,Mingu Lee,R. Memisevic,Hao Su","id":"8236010c2ecc94d826be6010ff187fdc000e7df6","summary":"This work proposes Natural Program, a natural language-based deductive reasoning format that enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps and significantly enhances the rigor and trustfulness of generated reasoning steps.","score":2},{"url":"https://www.semanticscholar.org/paper/50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","title":"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":4,"influentialCitationCount":0,"publicationDate":"07/07/2023","authors":"Yuxi Ma,Chi Zhang,Song-Chun Zhu","id":"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","summary":"This perspective paper comprehensively review existing evaluations of Large Language Models using both standardized tests and ability-oriented benchmarks to highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting.","score":2},{"url":"https://www.semanticscholar.org/paper/507acddb0b7f36b83fd7c8bff2f121eb506ac8fb","title":"Cumulative Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":117,"citationCount":22,"influentialCitationCount":5,"publicationDate":"08/08/2023","authors":"Yifan Zhang,Jingqin Yang,Yang Yuan,A. Yao","id":"507acddb0b7f36b83fd7c8bff2f121eb506ac8fb","summary":"A new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes to streamline the problem-solving process, rendering it both more manageable and effective.","score":2},{"url":"https://www.semanticscholar.org/paper/1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc","title":"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification","venue":"arXiv.org","year":2023,"referenceCount":29,"citationCount":37,"influentialCitationCount":0,"publicationDate":"15/08/2023","authors":"Aojun Zhou,Ke Wang,Zimu Lu,Weikang Shi,Sichun Luo,Zipeng Qin,Shaoqing Lu,Anya Jia,Linqi Song,Mingjie Zhan,Hongsheng Li","id":"1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc","summary":"The effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter is explored, and a novel and effective prompting method, explicit \\uline{c}ode-based \\ULine{s}elf-\\uline {v}erification~(CSV), is proposed to further boost the mathematical reasoning potential of GPN.","score":2},{"url":"https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","venue":"arXiv.org","year":2023,"referenceCount":178,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/08/2023","authors":"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang","id":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","summary":"This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","score":2},{"url":"https://www.semanticscholar.org/paper/62b4e06f5249d22e4a153ec4a2dc934c6a014372","title":"OWL: A Large Language Model for IT Operations","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":2,"influentialCitationCount":0,"publicationDate":"17/09/2023","authors":"Hongcheng Guo,Jian Yang,Jiaheng Liu,Liqun Yang,Linzheng Chai,Jiaqi Bai,Junran Peng,Xiaorong Hu,Chao Chen,Dongfeng Zhang,Xu Shi,Tieqiao Zheng,Liangfan Zheng,Bo Zhang,Ke Xu,Zhoujun Li","id":"62b4e06f5249d22e4a153ec4a2dc934c6a014372","summary":"The OWL is introduced, a large language model trained on the authors' collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback","venue":"arXiv.org","year":2023,"referenceCount":69,"citationCount":25,"influentialCitationCount":4,"publicationDate":"19/09/2023","authors":"Xingyao Wang,Zihan Wang,Jiateng Liu,Yangyi Chen,Lifan Yuan,Hao Peng,Heng Ji","id":"12b233752c7097ea6525622bed238ae2d2193c5a","summary":"MINT is introduced, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback, and repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making into a compact subset for efficient evaluation.","score":2},{"url":"https://www.semanticscholar.org/paper/cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9","title":"LPML: LLM-Prompting Markup Language for Mathematical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":20,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/09/2023","authors":"Ryutaro Yamauchi,Sho Sonoda,Akiyoshi Sannai,Wataru Kumagai","id":"cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9","summary":"This paper proposed a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL) and discovered that by prompting LLMs to generate structured text in XML-like markup language, it could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs.","score":2},{"url":"https://www.semanticscholar.org/paper/77b1f1c6d1658d120456b9046667cf009ceb39ce","title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":74,"citationCount":26,"influentialCitationCount":1,"publicationDate":"21/09/2023","authors":"L. Yu,Weisen Jiang,Han Shi,Jincheng Yu,Zhengying Liu,Yu Zhang,James T. Kwok,Zheng Li,Adrian Weller,Weiyang Liu","id":"77b1f1c6d1658d120456b9046667cf009ceb39ce","summary":"Experimental results on two popular benchmarks for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin.","score":2},{"url":"https://www.semanticscholar.org/paper/5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","title":"Qwen Technical Report","venue":"arXiv.org","year":2023,"referenceCount":164,"citationCount":80,"influentialCitationCount":16,"publicationDate":"28/09/2023","authors":"Jinze Bai,Shuai Bai,Yunfei Chu,Zeyu Cui,Kai Dang,Xiaodong Deng,Yang Fan,Wenhang Ge,Yu Han,Fei Huang,Binyuan Hui,Luo Ji,Mei Li,Junyang Lin,Runji Lin,Dayiheng Liu,Gao Liu,Chengqiang Lu,K. Lu,Jianxin Ma,Rui Men,Xingzhang Ren,Xuancheng Ren,Chuanqi Tan,Sinan Tan,Jianhong Tu,Peng Wang,Shijie Wang,Wei Wang,Shengguang Wu,Benfeng Xu,Jin Xu,An Yang,Hao Yang,Jian Yang,Jian Yang,Shusheng Yang,Shusheng Yang,Bowen Yu,Yu Bowen,Hongyi Yuan,Zheng Yuan,Jianwei Zhang,Xing Zhang,Yichang Zhang,Zhenru Zhang,Chang Zhou,Jingren Zhou,Xiaohuan Zhou,Tianhang Zhu","id":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","summary":"Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts, and includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques.","score":2},{"url":"https://www.semanticscholar.org/paper/a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":9,"influentialCitationCount":1,"publicationDate":"29/09/2023","authors":"Lifan Yuan,Yangyi Chen,Xingyao Wang,Y. Fung,Hao Peng,Heng Ji","id":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","summary":"CRAFT is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning, and achieves substantial improvements compared to strong baselines.","score":2},{"url":"https://www.semanticscholar.org/paper/d4bf36cbc5855ea87235d7a64f406717ac6aa3c9","title":"Large Language Models as Analogical Reasoners","venue":"arXiv.org","year":2023,"referenceCount":89,"citationCount":5,"influentialCitationCount":0,"publicationDate":"03/10/2023","authors":"Michihiro Yasunaga,Xinyun Chen,Yujia Li,Panupong Pasupat,J. Leskovec,Percy Liang,E. Chi,Denny Zhou","id":"d4bf36cbc5855ea87235d7a64f406717ac6aa3c9","summary":"Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, this approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem.","score":2},{"url":"https://www.semanticscholar.org/paper/b29134737a0c81c13d31fc0263b3c4d4f05ccb78","title":"Guiding Language Model Reasoning with Planning Tokens","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":3,"influentialCitationCount":2,"publicationDate":"09/10/2023","authors":"Xinyi Wang,Lucas Caccia,O. Ostapenko,Xingdi Yuan,Alessandro Sordoni","id":"b29134737a0c81c13d31fc0263b3c4d4f05ccb78","summary":"'planning tokens' are introduced at the start of each reasoning step, serving as a guide for the model, and these token embeddings are then fine-tuned along with the rest of the model parameters.","score":2},{"url":"https://www.semanticscholar.org/paper/e93562137240873bf1262e769dd9d73c2dcba858","title":"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization","venue":"arXiv.org","year":2023,"referenceCount":31,"citationCount":3,"influentialCitationCount":2,"publicationDate":"09/10/2023","authors":"Chengpeng Li,Zheng Yuan,Guanting Dong,Keming Lu,Jiancan Wu,Chuanqi Tan,Xiang Wang,Chang Zhou","id":"e93562137240873bf1262e769dd9d73c2dcba858","summary":"An investigation for data augmentation in math reasoning with large language models finds that MuggleMath is weak in out-of-domain math reasoning generalization to MATH, which suggest that augmentation on a single benchmark could not help with overall math reasoning performance.","score":2},{"url":"https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5","title":"Lemur: Harmonizing Natural Language and Code for Language Agents","venue":"arXiv.org","year":2023,"referenceCount":94,"citationCount":19,"influentialCitationCount":2,"publicationDate":"10/10/2023","authors":"Yiheng Xu,Hongjin Su,Chen Xing,Boyu Mi,Qian Liu,Weijia Shi,Binyuan Hui,Fan Zhou,Yitao Liu,Tianbao Xie,Zhoujun Cheng,Siheng Zhao,Lingpeng Kong,Bailin Wang,Caiming Xiong,Tao Yu","id":"8147cec9245d34d13732a08e915c920a1a499bb5","summary":"Lemur and Lemur-Chat are introduced, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either.","score":2},{"url":"https://www.semanticscholar.org/paper/3b918b15178bcc84fd22af5094fe1efbcd388e72","title":"Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":40,"citationCount":2,"influentialCitationCount":1,"publicationDate":"13/10/2023","authors":"Fanqi Wan,Xinting Huang,Tao Yang,Xiaojun Quan,Wei Bi,Shuming Shi","id":"3b918b15178bcc84fd22af5094fe1efbcd388e72","summary":"This work proposes Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs), built upon representative domain use cases.","score":2},{"url":"https://www.semanticscholar.org/paper/8868a6d452b06bf4ad33237d0f3952d895ca20e7","title":"Improving Large Language Model Fine-tuning for Solving Math Problems","venue":"arXiv.org","year":2023,"referenceCount":28,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/10/2023","authors":"Yixin Liu,Avi Singh,C. D. Freeman,John D. Co-Reyes,Peter J. Liu","id":"8868a6d452b06bf4ad33237d0f3952d895ca20e7","summary":"A fine-tuning recipe is designed that yields an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting and a multi-task sequential fine- Tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance.","score":2},{"url":"https://www.semanticscholar.org/paper/aac8cdd40b2bfd1b967f0e5ea6c01e93385169e7","title":"SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/10/2023","authors":"Xueliang Zhao,Xinting Huang,Wei Bi,Lingpeng Kong","id":"aac8cdd40b2bfd1b967f0e5ea6c01e93385169e7","summary":"A novel framework called SEGO is proposed to enhance LLMs' ability to solve mathematical problems by establishing a connection between the subgoal breakdown process and the probability of solving problems, and SEGO aims to identify better subgoals with theoretical guarantees.","score":2},{"url":"https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240","title":"Implicit Chain of Thought Reasoning via Knowledge Distillation","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/11/2023","authors":"Yuntian Deng,Kiran Prasad,Roland Fernandez,P. Smolensky,Vishrav Chaudhary,Stuart Shieber","id":"4411e6b32865933cab87696c2738cb7a204e4240","summary":"This work explores an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, the language model's internal hidden states are used to perform implicit reasoning, which enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain- of-thought.","score":2},{"url":"https://www.semanticscholar.org/paper/a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261","title":"StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2023","authors":"Chang Gao,Haiyun Jiang,Deng Cai,Shuming Shi,Wai Lam","id":"a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261","summary":"Results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning, commonsense reasoning, and symbolic reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/be130fe97c15048dd91cad438894fbed5a05365a","title":"Meta Prompting for AGI Systems","venue":"arXiv.org","year":2023,"referenceCount":128,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2023","authors":"Yifan Zhang","id":"be130fe97c15048dd91cad438894fbed5a05365a","summary":"This paper explores the formal definitions of Meta Prompting, sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications, and introduces Meta Prompting for prompting tasks, allowing LLMs to self-generate new prompts in a recursive, metaprogramming-like manner.","score":2},{"url":"https://www.semanticscholar.org/paper/dab4f70d75a04e62553e583f2450d9bb1f0ead46","title":"CLOMO: Counterfactual Logical Modification with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2023","authors":"Yinya Huang,Ruixin Hong,Hongming Zhang,Wei Shao,Zhicheng YANG,Dong Yu,Changshui Zhang,Xiaodan Liang,Linqi Song","id":"dab4f70d75a04e62553e583f2450d9bb1f0ead46","summary":"This study introduces a novel task, Counterfactual Logical Modification (CLOMO), and proposes an innovative evaluation metric, the LogicAware CounterfactUAL Score, to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem.","score":2},{"url":"https://www.semanticscholar.org/paper/d4346af837aa6c2bb4a341cfe9bd91862ea5910a","title":"Large Knowledge Model: Perspectives and Challenges","venue":"arXiv.org","year":2023,"referenceCount":80,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/12/2023","authors":"Huajun Chen","id":"d4346af837aa6c2bb4a341cfe9bd91862ea5910a","summary":"This article investigates the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, and proposes a five-``A'' principle to distinguish the concept of LKM.","score":2},{"url":"https://www.semanticscholar.org/paper/36f71673d9337b432babc51da77ef38b2070b5ed","title":"An LLM Compiler for Parallel Function Calling","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/12/2023","authors":"Sehoon Kim,Suhong Moon,Ryan Tabrizi,Nicholas Lee,Michael W. Mahoney,Kurt Keutzer,A. Gholami","id":"36f71673d9337b432babc51da77ef38b2070b5ed","summary":"LLMCompiler is introduced, which executes functions in parallel to efficiently orchestrate multi-function calling and automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2.","score":2},{"url":"https://www.semanticscholar.org/paper/6ac627f57b26354ab537734d820da4a6a7dde2c6","title":"CLadder: Assessing Causal Reasoning in Language Models","venue":"","year":2023,"referenceCount":100,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/12/2023","authors":"Zhijing Jin,Yuen Chen,Felix Leeb,Luigi Gresele,Ojasv Kamal,Zhiheng Lyu,Kevin Blin,Fernando Gonzalez Adauto,Max Kleiman-Weiner,Mrinmaya Sachan,Bernhard Scholkopf","id":"6ac627f57b26354ab537734d820da4a6a7dde2c6","summary":"This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al.","score":2},{"url":"https://www.semanticscholar.org/paper/c08be5d9c83ca9d004e999e4aa11dc967edeacc8","title":"MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation","venue":"","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/12/2023","authors":"Zhongshen Zeng,Pengguang Chen,Shu Liu,Haiyun Jiang,Jiaya Jia","id":"c08be5d9c83ca9d004e999e4aa11dc967edeacc8","summary":"This work introduces a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning, and addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents.","score":2},{"url":"https://www.semanticscholar.org/paper/c3d1832ed0444f75d44116fabbdda891aebc4b01","title":"LLaMA Pro: Progressive LLaMA with Block Expansion","venue":"arXiv.org","year":2024,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/01/2024","authors":"Chengyue Wu,Yukang Gan,Yixiao Ge,Zeyu Lu,Jiahao Wang,Ye Feng,Ping Luo,Ying Shan","id":"c3d1832ed0444f75d44116fabbdda891aebc4b01","summary":"A new post-pretraining method for LLMs with an expansion of Transformer blocks is proposed, efficiently and effectively improving the model's knowledge without catastrophic forgetting, yielding LLaMA Pro-8.3B, a versatile foundation model, excelling in general tasks, programming, and mathematics.","score":2},{"url":"https://www.semanticscholar.org/paper/7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks","venue":"","year":2024,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2024","authors":"Zhiruo Wang,Daniel Fried,Graham Neubig","id":"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","summary":"TROVE is presented, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox.","score":2},{"url":"https://www.semanticscholar.org/paper/ecc3415b74717b3f786760e12934a31b37d98312","title":"TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data","venue":"","year":2024,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2024","authors":"Fengbin Zhu,Ziyang Liu,Fuli Feng,Chao Wang,Moxin Li,Tat-seng Chua","id":"ecc3415b74717b3f786760e12934a31b37d98312","summary":"This work develops a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-wise Pipeline, and verified that the TAT-LLM model can outperform all baseline models.","score":2},{"url":"https://www.semanticscholar.org/paper/6ac2856b0192ec307e349406270ec40a6f7e14a4","title":"YODA: Teacher-Student Progressive Learning for Language Models","venue":"","year":2024,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2024","authors":"Jianqiao Lu,Wanjun Zhong,Yufei Wang,Zhijiang Guo,Qi Zhu,Wenyong Huang,Yanlin Wang,Fei Mi,Baojun Wang,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu","id":"6ac2856b0192ec307e349406270ec40a6f7e14a4","summary":"YODA is introduced, a novel teacher-student progressive learning framework that emulates the teacher-student education process to improve the efficacy of model fine-tuning and finds that training with curriculum learning further improves learning robustness.","score":2},{"url":"https://www.semanticscholar.org/paper/f3489dfe237b72ece429290f912afbddbb828f4e","title":"Improving Small Language Models’ Mathematical Reasoning via Mix Thoughts Distillation","venue":"","year":null,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Xunyu Zhu,Jian Li,Yong Liu,Can Ma,Weiping Wang","id":"f3489dfe237b72ece429290f912afbddbb828f4e","summary":"This work introduces Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs and proposes the Mix Thoughts Distillation (MTD) framework to enhance the reasoning performance of SLMs.","score":2},{"url":"https://www.semanticscholar.org/paper/4befd752d21a6231a9d930b1946177bd4cba30cb","title":"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach","venue":"International Conference \"Information Technology and Interactions\"","year":2021,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Andrii D. Nikolaiev,A. Anisimov","id":"4befd752d21a6231a9d930b1946177bd4cba30cb","summary":"An overview of methods for processing mathematical text problems and correspondent domain datasets is described and a new approach to estimate MWP solutions is proposed that could use more context around the problem.","score":2},{"url":"https://www.semanticscholar.org/paper/064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","title":": Leveraging Collective Human Intelligence to Study Large Language Models","venue":"","year":2023,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Sebastin Santy,Ayana Bharadwaj,Sahith N. Dambekodi,Alex Albert,Cathy Yuan,Ranjay Krishna","id":"064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","summary":"L EET P ROMPT automatically evaluates human-LLM interactions to provide insights about both LLMs as well as human-interaction behavior, and finds that people use more diverse instruction strategies than these auto-* mechanisms.","score":2},{"url":"https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","venue":"Neural Information Processing Systems","year":2022,"referenceCount":118,"citationCount":2693,"influentialCitationCount":424,"publicationDate":"28/01/2022","authors":"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,F. Xia,Quoc Le,Denny Zhou","id":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","summary":"Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation","venue":"arXiv.org","year":2022,"referenceCount":182,"citationCount":24,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Vaibhav Mavi,Anubhav Jangra,A. Jatowt","id":"c9f48406851954cb098911eccb4124ea5f966675","summary":"A general and formal definition of MHQA task is provided, the existing attempts to this highly interesting, yet quite challenging task are organized and summarized, and the best methods to createMHQA datasets are outlined.","score":2},{"url":"https://www.semanticscholar.org/paper/d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers","venue":"arXiv.org","year":2022,"referenceCount":81,"citationCount":9,"influentialCitationCount":0,"publicationDate":"31/05/2022","authors":"S. S. Sundaram,Sairam Gurajada,M. Fisichella,Deepak P,Savitha Sam Abraham","id":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","summary":"This paper critically examine the various models that have been developed for solving word problems, their pros and cons and the challenges ahead, and endeavours to provide a road-map for future math word problem research.","score":2},{"url":"https://www.semanticscholar.org/paper/3ce8c07349d91bb3f022a211be36e98eef0e1046","title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems","venue":"arXiv.org","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Keyur Faldu,Amit P. Sheth,Prashant Kikani,Darshan Patel","id":"3ce8c07349d91bb3f022a211be36e98eef0e1046","summary":"A novel model MMTM that leverages multi-tasking and multi-decoder during pre-training that achieves better mathematical reasoning ability and generalisability, which is demonstrated by outperforming the best state of the art baseline models from Seq2Seq, GTS, and Graph2Tree with a relative improvement on an adversarial challenge dataset SVAMP.","score":2},{"url":"https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","title":"Language models show human-like content effects on reasoning","venue":"arXiv.org","year":2022,"referenceCount":113,"citationCount":94,"influentialCitationCount":6,"publicationDate":"14/07/2022","authors":"Ishita Dasgupta,Andrew Kyle Lampinen,Stephanie C. Y. Chan,Antonia Creswell,D. Kumaran,James L. McClelland,Felix Hill","id":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","summary":"Evaluated state of the art large language models, as well as humans, are evaluated and it is found that the language models reflect many of the same patterns observed in humans, and models answer more accurately when the semantic content of a task supports the logical inferences.","score":2},{"url":"https://www.semanticscholar.org/paper/2fe6060ced80c1c245a718e6188b6516207bf0a8","title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":46,"citationCount":13,"influentialCitationCount":3,"publicationDate":"30/09/2022","authors":"Rindranirina Ramamonjison,Haley Li,Timothy T. Yu,Shiqi He,Vishnu Rengan,Amin Banitalebi-Dehkordi,Zirui Zhou,Yong Zhang","id":"2fe6060ced80c1c245a718e6188b6516207bf0a8","summary":"An augmented intelligence system for simplifying and enhancing the modeling experience for operations research is described that receives a suggested formulation of an optimization problem based on its description and enables the users to validate and edit the suggestions.","score":2},{"url":"https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation","venue":"International Conference on Learning Representations","year":2022,"referenceCount":82,"citationCount":50,"influentialCitationCount":3,"publicationDate":"11/10/2022","authors":"Ruibo Liu,Jason Wei,S. Gu,Te-Yen Wu,Soroush Vosoughi,Claire Cui,Denny Zhou,Andrew M. Dai","id":"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","summary":"Mind's Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/b23a8493f384a52adf22d3c70c5827fd1a6ca42d","title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":51,"citationCount":7,"influentialCitationCount":1,"publicationDate":"21/10/2022","authors":"Wenqi Zhang,Yongliang Shen,Yanna Ma,Xiaoxia Cheng,Zeqi Tan,Qingpeng Nong,Weiming Lu","id":"b23a8493f384a52adf22d3c70c5827fd1a6ca42d","summary":"Experiments show the proposed multi-view consistent contrastive learning approach significantly outperforms the existing baselines, especially on complex problems, and can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.","score":2},{"url":"https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","venue":"arXiv.org","year":2022,"referenceCount":49,"citationCount":255,"influentialCitationCount":38,"publicationDate":"22/11/2022","authors":"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen","id":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","summary":"Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoTA performance on all math problem datasets and near-SoTAperformance on financial datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":65,"citationCount":16,"influentialCitationCount":1,"publicationDate":"23/11/2022","authors":"K. Shridhar,Jakub Macina,Mennatallah El-Assady,Tanmay Sinha,Manu Kapur,Mrinmaya Sachan","id":"e6745fb621481ccb0ed53c267a37292e499c1b42","summary":"This work explores the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving and proposes various guided question generation schemes based on input conditioning and reinforcement learning.","score":2},{"url":"https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f","title":"Distilling Reasoning Capabilities into Smaller Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":43,"citationCount":28,"influentialCitationCount":3,"publicationDate":"01/12/2022","authors":"K. Shridhar,Alessandro Stolfo,Mrinmaya Sachan","id":"8fd462f6248d5e3f1b6602697c09489086b5655f","summary":"This work proposes an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps, and uses this to train a combination of two small distilled models.","score":2},{"url":"https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":74,"citationCount":99,"influentialCitationCount":15,"publicationDate":"20/12/2022","authors":"Namgyu Ho,Laura Schmid,Se-Young Yun","id":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","summary":"This paper uses very large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude, and proposes Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tunes smaller models.","score":2},{"url":"https://www.semanticscholar.org/paper/27f0ce04403158b61328716ae4aaab5840c0d123","title":"Batch Prompting: Efficient Inference with Large Language Model APIs","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":81,"citationCount":21,"influentialCitationCount":2,"publicationDate":"19/01/2023","authors":"Zhoujun Cheng,Jungo Kasai,Tao Yu","id":"27f0ce04403158b61328716ae4aaab5840c0d123","summary":"Batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time, is proposed, which reduces both token and time costs while retaining downstream performance.","score":2}]}